{"timestamp": 1758437878.065402, "content": {"success": true, "result": {"source_url": "https://www.nexgencloud.com/blog/performance-benchmarks/nvidia-blackwell-vs-nvidia-hopper-a-detailed-comparison", "article_text": "We have been made awareof afraudulent third-party offeringof shares inNexGen Cloudby an individual purporting to work forLyxor Asset Management.If you have been approached tobuy shares in NexGen Cloud, westrongly adviseyouverify its legitimacy.\nTo do so, contact our Investor Relations team at[email protected]. We take such matters seriously and appreciate your diligence to ensure the authenticity of any financial promotions regarding NexGen Cloud.\nAI Superclusters at Scale\nThe Ultimate AI Cloud Platform\nShaping the Future of AI & Technology\nDiscover the AI Supercloud\nNVIDIA HGX H100\nNVIDIA GB200 NVL72\nNVIDIA HGX H200\nVisit Site\nNVIDIA H100 SXM On-Demand\nOn-Demand GPU Pricing\nNVIDIA H100 PCIe On-Demand\nDocumentation\nNVIDIA A100 On-Demand\nDiscover the story of NexGen Cloud.\nExplore NexGen Cloud's mission and values.\nMeet NexGen Cloud's Leadership Team.\nA message from our CEO, Chris Starkey.\nLearn about our commitment to sustainability.\nExplore career opportunities and join our team.\nLearn more about our existing partnerships\nBecome a Channel Partner\nRead blogs on Thought Leadership.\nLatest updates and announcements.\nPerformance Benchmarks\nBusiness Case Studies\nThought Leadership\nInnovation\nAll Blogs\nNews\nAll Events\nPress Coverage\nAI SUPERCLOUD\nHyperstack\nNexGen Labs\nOur Organization\nBLOG\nNews and Events\nOctober 1, 2024\n5 min read\nUpdated on 12 Sep 2025\nWritten by\nTechnical Copywriter, NexGen cloud\nShare this post\nTable of contents\nIn our blog, we will discuss the key differences between NVIDIA Hopper and Blackwell architectures, comparing their performance, features, and suitability for AI and HPC workloads. NVIDIA Hopper, launched in 2022, introduced powerful AI capabilities with 80 billion transistors and HBM3 memory, excelling in LLMs and scientific computing. Blackwell, released in 2024, delivers a massive leap with 208 billion transistors, second-gen Transformer Engine, and 10 TB/s interconnect, making it ideal for generative AI and large-scale simulations. We’ll explore how these architectures impact AI acceleration and real-world applications.\n\"We created a processor for the generative AI era,\"said NVIDIA’s CEO Jensen Huang at GTC 2024, as he announced the highly anticipated NVIDIA Blackwell chip. This announcement broke records in the AI space. Designed to meet the demands of the most complex AI models and data-intensive workloads, Blackwell delivers a 2.5x performance boost over its previous-gen architecture NVIDIA Hopper.\nWhile NVIDIA’s Hopper architecture set a new standard for AI and accelerated computing in 2022, Blackwell takes things to another level. But what makes each of these architectures unique and how do they stack up against each other? In this blog, we’ll explore the key differences between NVIDIA Hopper and Blackwell, and which is best suited for your AI and HPC projects.\nThe table below shows the NVIDIA Hopper vs NVIDIA Blackwell specs:\nFeature\nNVIDIA Hopper Architecture\nNVIDIA Blackwell Architecture\nTransistor Count\n80 billion transistors\n208 billion transistors\nManufacturing Process\nTSMC 4N process\nCustom-built TSMC 4NP process\nTransformer Engine\nFirst-generation\nSecond-generation\nDecompression Engine\nNo\nYes\nEnergy Efficiency\nImproved over the previous generation\n25x more energy-efficient than Hopper\nInterconnect Technology\nFourth-generation NVLink\nFifth-generation NVLink\nChip-to-Chip Interconnect\n900 GB/s\n10 TB/s\nApplications\nGenerative AI, LLMs, Data processing, Quantum computing\nAccelerated Computing, AI, LLM\nNVIDIA Hopper was launched in 2022 and named after Grace Hopper, a pioneering computer scientist and U.S. Navy rear admiral who was instrumental in the development of computer programming languages. The NVIDIA Hopper architecture excels in tasks like transformer-based AI models, large-scale language models (LLMs), and scientific computing. With 80 billion transistors and HBM3 memory, the H100 GPU offers up to 4 petaflops of AI performance, making it ideal for enterprises and research institutions working on AI, large-scale data centres, and scientific simulations. Hopper’s Transformer Engine is key for accelerating tensor operations, essential for deep learning tasks.\nBeyond performance, Hopper also introduced Confidential Computing capabilities, which offer enhanced data security, allowing enterprises to protect sensitive information while performing complex computations.\nNVIDIA Blackwell was released in 2024 and named after David Harold Blackwell, a renowned statistician and mathematician whose work in probability theory and dynamic programming has had a lasting impact on computational sciences. Blackwell’s contributions to the field of mathematical statistics align with the architecture’s focus on generative AI, LLMs, and data-centric workloads.\nThe NVIDIA GB200 GPU from the Blackwell series boasts 208 billion transistors and HBM3e memory, offering a massive leap in performance compared to the Hopper series. With up to 20 petaflops of AI performance, Blackwell is designed to handle the most demanding computational tasks, such as training large AI models, running complex simulations, and accelerating generative AI applications.\nOne of Blackwell's standout features is its dedicated decompression engine, which speeds up data processing by up to 800 GB/s—making it 6x faster than Hopper when dealing with large datasets. Blackwell also enhances Confidential Computing, ensuring a secure and efficient environment for sensitive AI workloads, while introducing breakthroughs in quantum simulations and scientific research, making it the next frontier in AI and HPC computing.\nThe performance of NVIDIA Hopper architecture and Blackwell benchmarks shows the significant advancements each architecture brings to cater to different aspects of AI and high-performance computing (HPC).\nThe NVIDIA H100 Tensor Core GPU is a powerhouse for AI inference, training, and accelerated computing. Leveraging innovations from the Hopper architecture, the H100 is equipped with 80 billion transistors and HBM3 memory, delivering breakthrough performance and scalability for AI models, HPC tasks, and enterprise data centres. The H100 features a Transformer Engine with FP8 precision, offering up to 4x faster training compared to previous generations for large models like Llama 3.\nKey capabilities of the NVIDIA Hopper Series H100 GPU include:\nThe NVIDIA H100 GPUs are designed to supercharge large language models (LLMs) like Llama 3, offering up to 5x faster inference compared to the NVIDIA A100 and with enterprise-ready features like NVIDIA Confidential Computing and NVIDIA AI Enterprise software, it simplifies AI deployment with high levels of security, scalability, and manageability.\nThe NVIDIA Blackwell GB200 NVL/72 36 GPU shows a major leap in generative AI and accelerated computing. Built using 208 billion transistors and featuring HBM3e memory, the NVIDIA GB200 NVL/72 36 provides massive computational power for AI training, inference, and large-scale simulations.\nKey NVIDIA Blackwell features of NVIDIA GB200 NVL/72 36 include:\nWithup to 20 petaflopsof AI performance, Blackwell is ideal for generative AI models, LLMs, and high-performance scientific computing.\nAs an official NVIDIA NCP Partner, we integrate the latest NVIDIA technologies with flexible configuration options so you can scale your AI and HPC projects. AtAI Supercloud, we offer the following cutting-edge hardware:\nNVIDIA HGX H100\nNVIDIA HGX H200\nThe NVIDIA Hopper series, including the NVIDIA HGX H100 and NVIDIA HGX H200, are available on the AI Supercloud for deployment. These GPUs are designed for high-performance AI and accelerated computing workloads, offering industry-leading capabilities for tasks like large language model (LLM) inference, HPC, and enterprise AI solutions.\nYou can reserve the NVIDIA HGX H100 or NVIDIA HGX H200 and take advantage of our customised solutions tailored to your workload requirements. With fully managed services, on-demand scalability, and MLOps support, the AI Supercloud is optimized to help you deploy your AI projects faster and more efficiently.\nBook a call today to explore how NVIDIA Hopper GPUs can accelerate your AI and HPC tasks, and to discuss custom configurations for your specific needs.\nBook a Discovery Call\nThe highly anticipated NVIDIA Blackwell chip GB200 NVL72/36 is expected to be available by the end of 2025. These cutting-edge GPUs are designed to deliver exceptional performance for AI and high-performance computing (HPC) workloads.We are one of the first Elite Cloud Partners in the NVIDIA Partner Network to offer NVIDIA Blackwell platform-powered compute services.You can reserve the NVIDIA GB200 NVL72/36 in advance on the AI Supercloud and secure early access to the fastest performance for your AI projects. By reserving now, you’ll be among the first to leverage Blackwell’s unmatched capabilities in generative AI and large-scale model training.\nBook a call today to discuss how our bespoke solutions and managed services for NVIDIA GB200 NVL72/36 can help take your AI initiatives to the next level.\nBook a Discovery Call\nYes, NVIDIA Blackwell is up to 2.5 times faster than Hopper, offering a performance boost through advancements like the second-generation Transformer Engine, a decompression engine and a much faster chip-to-chip interconnect speed.\nThe NVIDIA Hopper series, including the NVIDIA HGX H100 and NVIDIA HGX H200, are available on the AI Supercloud for deployment.Schedule a call with our Solutions Consultanttoday to explore how NVIDIA Hopper GPUs can accelerate your AI and HPC tasks and to discuss custom configurations for your specific needs.\nThe NVIDIA H100 features 80 billion transistors, HBM3 memory and excels in AI inference and HPC with up to 4 petaflops of AI performance. While the NVIDIA GB200 NVL72  offers 208 billion transistors, HBM3e memory and a 2.5x performance boost with up to 20 petaflops, ideal for generative AI and large-scale model training.\nNVIDIA Blackwell features 208 billion transistors, HBM3e memory, a second-generation Transformer Engine, and a decompression engine for ultra-fast data processing. Its 10 TB/s chip-to-chip interconnect and enhanced Confidential Computing make it ideal for generative AI, large-scale models, and HPC workloads.\nNVIDIA Blackwell GPUs such as the NVIDIA GB200 NVL72/36, are expected to be available by the end of 2025. Early access reservations are available through AI Supercloud for those wanting to leverage its cutting-edge capabilities in generative AI and HPC.\nShare this post\nSubscribe to our newsletter for the latest updates and insights.\nStay updated with our latest articles.\nAI Supercloud will use NVIDIA Blackwell platform to drive enhanced efficiency, reduced costs and ...\nMarch 19, 2024\n5 min read\nAI Net Zero Collaboration to Power European AI London, United Kingdom – 26th February 2024; NexGen ...\nFebruary 27, 2024\n5 min read\nNexGen Cloud’s Hyperstack Platform and AI Supercloud Are Leveraging WEKA’s Data Platform Software To ...\nJanuary 31, 2024\n5 min read\nThe Hyperstack collaboration significantly increases the capacity and availability of AI infrastructure ...\nJanuary 25, 2024\n5 min read\nNexGen Cloud, the sustainable Infrastructure-as-a-Service provider, has today launched Hyperstack, an ...\nAugust 31, 2023\n5 min read\nX [Twitter]\nLinkedIn\nYoutube\nFacebook\nNexGen Cloud is the AI Factory – accelerating the future with the industry’s best GPUs in large-scale sovereign AI Cloud environments.\nGet the latest updates and exclusive offers.\nUnited Kingdom Address(Head office)\nLG 02/03, 1st Floor24 Greville StLondon EC1N 8SS\nRegistered Office\n6th Floor, 99 Gresham Street,London, EC2V 7NGUnited Kingdom\nSpain Address\nCtra NACIONAL 340, KM 176Local C-12,Marbella 29600 Malaga\nProducts\nQuicklinks\n@Copyright NexGen Cloud Ltd. 2025. All Rights Reserved."}, "error": null, "resultType": "object"}}